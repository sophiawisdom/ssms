.version 7.4
.target sm_80
.address_size 64

// N = 16,384
// ROOT_N = 128
// BATCH_SIZE = 16
// X is {BATCH_SIZE, N} or {16, 16384}
// w1_bfly is {ROOT_N, ROOT_N, ROOT_N} or {128, 128, 128}
// we write back out to X
// we need 128 MMAs of size M=16 K=16 N=8 in order to do a full M=16 K=128 N=128 matmul

.visible .entry monarch_kernel(
        .param .u64 x_ptr_param,
        .param .u64 w1_bfly_ptr_param,
        .param .u64 out_ptr_param
)
.maxntid 32, 8, 1
{
    .shared .b16 shared_w1[128][128]; // 32768 bytes
    .shared .b16 shared_x[16][128]; // 4096 bytes

    .reg .u32 tid_x;
    mov.u32 tid_x, %tid.x;
    .reg .u32 tid_y;
    mov.u32 tid_y, %tid.y;
    .reg .u32 overall_thread_num;
    mul.lo.u32 overall_thread_num, tid_y, 32; // 32 threads
    add.u32 overall_thread_num, overall_thread_num, tid_x;
    .reg .u64 shared_x_offset_64;
    mul.wide.u32 shared_x_offset_64, overall_thread_num, 16; // load 16 bytes per thread
    .reg .u64 shared_x_ptr_register;
    mov.u64 shared_x_ptr_register, shared_x;
    .reg .u64 shared_x_store_ptr;
    add.u64 shared_x_store_ptr, shared_x_ptr_register, shared_x_offset_64;
    .reg .u64 global_x_ptr;
    ld.param.u64 global_x_ptr, [x_ptr_param];
    .reg .u64 global_x_load_ptr;
    add.u64 global_x_load_ptr, global_x_ptr, shared_x_offset_64;

    // TODO: we kind of want cs and not ca?
    // TODO FOR REAL: DO THE PERMUTATION HERE, WHICH WE CURRENTLY DON'T DO
    cp.async.ca.shared.global [shared_x_store_ptr], [global_x_load_ptr], 16;

    .reg .u64 global_w1_ptr;
    ld.param.u64 global_w1_ptr, [w1_bfly_ptr_param];
    .reg .u64 shared_w1_ptr_register;
    mov.u64 shared_w1_ptr_register, shared_w1;
    .reg .u64 shared_w1_store_ptr;
    add.u64 shared_w1_store_ptr, shared_w1_ptr_register, shared_x_offset_64; // each thread loads a different part, so need offsets...
    // TODO: make sure global_w1_load_ptr gets the correct matrix
    .reg .u64 global_w1_load_ptr;
    add.u64 global_w1_load_ptr, global_w1_ptr, shared_x_offset_64;

    cp.async.ca.shared.global [shared_w1_store_ptr + 0], [global_w1_load_ptr + 0], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 4096], [global_w1_load_ptr + 4096], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 8192], [global_w1_load_ptr + 8192], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 12288], [global_w1_load_ptr + 12288], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 16384], [global_w1_load_ptr + 16384], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 20480], [global_w1_load_ptr + 20480], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 24576], [global_w1_load_ptr + 24576], 16;
    cp.async.commit_group;
    cp.async.ca.shared.global [shared_w1_store_ptr + 28672], [global_w1_load_ptr + 28672], 16;
    cp.async.commit_group;

    // np.sum([a[:,i:i+16] @ b[i:i+16,0:8] for i in range(0, 128, 16)], axis=0)

    // if we define an A matrix as `a = np.arange(16 * 128).reshape((16, 128))`
    // for the x matrix, what we need is a[:,:16] or equivalently a[:16,:16]. However, we cannot *directly* load a 16x16 matrix.
    // we instead load 4 8x8 matrices: a[:8, :8], a[:8, 8:16], a[8:16, :8], and a[8:16, 8:16]. The row indices for a[:8, :8] must
    // be in t0-t7, for a[:8, 8:16] in t8-16, etc. You can see the indices we need for t0-t7 with a[:8,:8][:,0], t8-t15 with
    // a[:8,8:16][:,0], t16-23 with a[8:16,:8][:,0], and t24-31 with a[8:16, 8:16][:,0].
    // the bit arithmetic you need to achieve the desired indices is:
    // ((np.arange(32) & 8) << 0) + (((np.arange(32) & 23) - ((np.arange(32) & 16) >> 1)) << 7).
    // to achive the desired pointers, you multiply everything by two, so:
    // ((np.arange(32) & 8) << 1) + (((np.arange(32) & 23) - ((np.arange(32) & 16) >> 1)) << 8)
    // when friends saw this, they said "jesus christ"

    // This is the same for every warp. As we go down and compute subsequent parts of X, we now load a[:,16:32], a[:,32:48] etc.
    // which is just a pointer increment from a[,0:16].

    // ((np.arange(32) & 8) << 1)
    .reg .u32 eight_part;
    and.b32 eight_part, tid_x, 8;
    shl.b32 eight_part, eight_part, 1;

    .reg .u32 twenty_three_part;
    and.b32 twenty_three_part, tid_x, 23;

    .reg .u32 sixteen_part;
    and.b32 sixteen_part, tid_x, 16;
    shr.b32 sixteen_part, sixteen_part, 1;

    .reg .u32 sixteen_twenty_three_part;
    sub.u32 sixteen_twenty_three_part, twenty_three_part, sixteen_part;
    shl.b32 sixteen_twenty_three_part, sixteen_twenty_three_part, 8;

    .reg .u32 ldmatrix_offset;
    add.u32 ldmatrix_offset, sixteen_twenty_three_part, eight_part;

    // calculate x_ldmatrix_ptr
    .reg .u64 x_ldmatrix_offset_64;
    cvt.u64.u32 x_ldmatrix_offset_64, ldmatrix_offset;
    .reg .u64 x_ldmatrix_ptr;
    add.u64 x_ldmatrix_ptr, shared_x_ptr_register, x_ldmatrix_offset_64;

    .reg .u32 warp_id;
    mov.u32 warp_id, %tid.y;
    .reg .u32 warp_weights_offset;
    shl.b32 warp_weights_offset, warp_id, 4; // multiply by 16
    // we want b_indices[0:16, warp_id*16:(warp_id+1):16] and then b_indices[16:32, warp_id*16:(warp_id+1):16]
    .reg .u32 weights_ldmatrix_offset;
    add.u32 weights_ldmatrix_offset, ldmatrix_offset, warp_weights_offset;
    // add.u32 weights_ldmatrix_offset, ldmatrix_offset, 0;

    .reg .u64 weights_ldmatrix_offset_64;
    cvt.u64.u32 weights_ldmatrix_offset_64, weights_ldmatrix_offset;
    .reg .u64 weights_ldmatrix_ptr;
    add.u64 weights_ldmatrix_ptr, shared_w1_ptr_register, weights_ldmatrix_offset_64;

    .reg .f32 out_<8>;
    mov.f32 out_0, 0f00000000;
    mov.f32 out_1, 0f00000000;
    mov.f32 out_2, 0f00000000;
    mov.f32 out_3, 0f00000000;
    mov.f32 out_4, 0f00000000;
    mov.f32 out_5, 0f00000000;
    mov.f32 out_6, 0f00000000;
    mov.f32 out_7, 0f00000000;

    .reg .b32 x_<4>;
    mov.f32 x_0, 0f00000000;
    mov.f32 x_1, 0f00000000;
    mov.f32 x_2, 0f00000000;
    mov.f32 x_3, 0f00000000;
    .reg .b32 weights_<4>;

    bar.sync 0;

    cp.async.wait_all; // TODO: structure the cp async so we don't have to do this

    bar.sync 1;

    // cp.async.wait_group 7;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 0];
    // TODO FOR REAL: make sure that the first w1 cp async gets the first row, second second row, etc.
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 0];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 6;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 32];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 4096];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 5;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 64];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 8192];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 4;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 96];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 12288];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 3;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 128];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 16384];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 2;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 160];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 20480];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 1;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 192];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 24576];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    // cp.async.wait_group 0;
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 224];
    ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {weights_0, weights_1, weights_2, weights_3}, [weights_ldmatrix_ptr + 28672];
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_0, x_1, x_2, x_3}, {weights_0, weights_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_4, out_5, out_6, out_7}, {x_0, x_1, x_2, x_3}, {weights_2, weights_3}, {out_4, out_5, out_6, out_7};

    .reg .b32 out_small_<4>;
    cvt.rn.bf16x2.f32 out_small_0, out_1, out_0;
    cvt.rn.bf16x2.f32 out_small_1, out_3, out_2;
    cvt.rn.bf16x2.f32 out_small_2, out_5, out_4;
    cvt.rn.bf16x2.f32 out_small_3, out_7, out_6;

    .reg .u64 out_ptr_reg;
    ld.param.u64 out_ptr_reg, [out_ptr_param];

    .reg .u32 withinrow_offset;
    and.b32 withinrow_offset, tid_x, 3;
    shl.b32 withinrow_offset, withinrow_offset, 2;

    .reg .u32 betweenrow_offset;
    and.b32 betweenrow_offset, tid_x, 28;
    shl.b32 betweenrow_offset, betweenrow_offset, 6;

    .reg .u32 warp_offset;
    shl.b32 warp_offset, tid_y, 5;

    .reg .u32 out_offset;
    add.u32 out_offset, withinrow_offset, betweenrow_offset;
    add.u32 out_offset, out_offset, warp_offset;

    .reg .u64 out_offset_64;
    cvt.u64.u32 out_offset_64, out_offset;
    .reg .u64 out_ptr_matrix_store;
    add.u64 out_ptr_matrix_store, out_ptr_reg, out_offset_64;

    // out_small_0 is two values of c_indices[:8,:warp_id*8],
    // out_small_1 is two values of c_indices[8:16,:warp_id*8]
    // out_small_2 is two values of c_indices[:8,warp_id*8:warp_id*8+8]
    // out_small_3 is two values of c_indices[8:16,warp_id*8:warp_id*8+8]

    // TODO: permute back here!
    st.global.b32 [out_ptr_matrix_store + 0], out_small_0;
    st.global.b32 [out_ptr_matrix_store + 2048], out_small_1;
    st.global.b32 [out_ptr_matrix_store + 16], out_small_2;
    st.global.b32 [out_ptr_matrix_store + 2064], out_small_3;

    /*
    .reg .u64 x_global_ptr;
    add.u64 x_global_ptr, out_ptr_reg, x_ldmatrix_offset_64;
    st.global.b32 [x_global_ptr + 0], x_0;
    st.global.b32 [x_global_ptr + 4], x_1;
    st.global.b32 [x_global_ptr + 8], x_2;
    st.global.b32 [x_global_ptr + 12], x_3;
    */

    /*
    ldmatrix.sync.aligned.m8n8.x4.shared.b16 {x_0, x_1, x_2, x_3}, [x_ldmatrix_ptr + 0];

    st.global.b32 [out_ptr_reg + 0], x_0;
    st.global.b32 [out_ptr_reg + 16], x_1;
    st.global.b32 [out_ptr_reg + 2048], x_2;
    st.global.b32 [out_ptr_reg + 2064], x_3;
    */

    ret;
}