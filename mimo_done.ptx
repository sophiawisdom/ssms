//
// Generated by Sophia Wisdom
//

.version 7.5
.target sm_80
.address_size 64

.visible .entry mimo_kernel(
        .param .u64 u_ptr_param,
        .param .u64 a_ptr_param,
        .param .u64 b_ptr_param,
        .param .u64 c_ptr_param,
        .param .u64 d_ptr_param,
        .param .u64 output_ptr_param,
        .param .u64 sequence_length_param
)

.maxntid 256, 1, 1
{
    .reg .b64 u_ptr;
    .reg .b64 a_ptr;
    .reg .b64 b_ptr;
    .reg .b64 c_ptr;
    .reg .b64 d_ptr;
    .reg .b64 output_ptr;
    .reg .u64 sequence_length;

    .reg .pred should_we_loop;

    ld.param.u64 u_ptr, [u_ptr_param]; // Sequence is of shape {SEQUENCE_LENGTH, BATCH_SIZE, L}. This isn't correct for layer 2, but we can ignore for now.
    ld.param.u64 a_ptr, [a_ptr_param]; // A is of shape {N_HEADS, N} AND IS IN FLOAT32 AND NOT BF16
    ld.param.u64 b_ptr, [b_ptr_param]; // B is of shape {N_HEADS, N,L}
    ld.param.u64 c_ptr, [c_ptr_param]; // C is of shape {N_HEADS, L,N}
    ld.param.u64 d_ptr, [d_ptr_param]; // D is of shape {N_HEADS, L,L}
    ld.param.u64 d_ptr, [d_ptr_param]; // D is of shape {N_HEADS, L,L}
    ld.param.u64 output_ptr, [output_ptr_param];
    ld.param.u64 sequence_length, [sequence_length_param];

    .reg .u16 sequence_length_small;
    cvt.u16.u64 sequence_length_small, sequence_length;

    .reg .u32 thread_id;
    .reg .u32 block_id;
    .reg .u32 lane_id;
    mov.u32 thread_id, %tid.x;
    mov.u32 block_id, %ctaid.x;
    mov.u32 lane_id, %laneid;

    // N=64 L=8, so multiply our block_id (which is our head) by N*L*2 bytes.
    .reg .u64 head_bc_offset;
    mul.wide.u32 head_bc_offset, block_id, 1024;
    // N=64, so multiply our block_id (which is our head) by N*4 bytes.
    .reg .u64 head_a_offset;
    mul.wide.u32 head_a_offset, block_id, 256;
    // L=8, so multiply our block_id (which is our head) by L*L*2 bytes.
    .reg .u64 head_d_offset;
    mul.wide.u32 head_d_offset, block_id, 128;
    // B=16 L=8, so multiply our block_id (which is our head) by B and L by 2 bytes.
    .reg .u64 head_in_out_offset;
    mul.wide.u32 head_in_out_offset, block_id, 256;

    .reg .u64 thread_id_times_2;
    mul.wide.u32 thread_id_times_2, thread_id, 2;
    .reg .f32 thread_id_times_2_float;
    cvt.rz.f32.u64 thread_id_times_2_float, thread_id_times_2;
    .reg .f32 thread_id_times_2_plus_1_float;
    add.f32 thread_id_times_2_plus_1_float, thread_id_times_2_float, 1.0;
    .reg .u64 thread_id_times_4;
    mul.wide.u32 thread_id_times_4, thread_id, 4;
    .reg .u64 thread_id_times_8;
    mul.wide.u32 thread_id_times_8, thread_id, 8;
    .reg .u64 thread_id_times_16;
    mul.wide.u32 thread_id_times_16, thread_id, 16;

    .reg .u64 d_ptr_for_thread;
    add.u64 d_ptr_for_thread, d_ptr, head_d_offset; // calculate d_ptr[head]
    add.u64 d_ptr_for_thread, d_ptr_for_thread, thread_id_times_4; // calculate d_ptr[head][threadrow]
    .reg .b32 d_fragment;
    ld.global.L1::no_allocate.b32 {d_fragment}, [d_ptr_for_thread];

    .reg .u64 c_ptr_for_thread;
    add.u64 c_ptr_for_thread, c_ptr, head_bc_offset; // calculate c_ptr[head]
    add.u64 c_ptr_for_thread, c_ptr_for_thread, thread_id_times_4;
    .reg .b32 c_<8>;
    // We need to load C in column-major (N, L) format, which is equivalent to row-major (L, N) format.
    ld.global.L1::no_allocate.b32 {c_0}, [c_ptr_for_thread]; // &C[0][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_1}, [c_ptr_for_thread+128]; // &C[8][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_2}, [c_ptr_for_thread+256]; // &C[16][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_3}, [c_ptr_for_thread+384]; // &C[24][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_4}, [c_ptr_for_thread+512]; // &C[32][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_5}, [c_ptr_for_thread+640]; // &C[40][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_6}, [c_ptr_for_thread+768]; // &C[48][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {c_7}, [c_ptr_for_thread+896]; // &C[56][0-1] + threadId*2

    .reg .u64 b_ptr_for_thread;
    add.u64 b_ptr_for_thread, b_ptr, head_bc_offset; // calculate b_ptr[head]
    add.u64 b_ptr_for_thread, b_ptr_for_thread, thread_id_times_4;
    .reg .b32 b_<8>;
    // B needs to be (L=8, N=64) but needs to be in column-major format which i think is equivalent to (N=64, L=8) row-major ordering, which is how it's stored.
    ld.global.L1::no_allocate.b32 {b_0}, [b_ptr_for_thread]; // &B[0][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_1}, [b_ptr_for_thread+128]; // &B[8][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_2}, [b_ptr_for_thread+256]; // &B[16][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_3}, [b_ptr_for_thread+384]; // &B[24][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_4}, [b_ptr_for_thread+512]; // &B[32][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_5}, [b_ptr_for_thread+640]; // &B[40][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_6}, [b_ptr_for_thread+768]; // &B[48][0-1] + threadId*2
    ld.global.L1::no_allocate.b32 {b_7}, [b_ptr_for_thread+896]; // &B[56][0-1] + threadId*2

    .reg .u64 thread_id_sdf;
    cvt.u64.u32 thread_id_sdf, thread_id;
    and.b64 thread_id_sdf, thread_id_sdf, 3;
    shl.b64 thread_id_sdf, thread_id_sdf, 3;
    // We load more a's than we would ideally like because of the way the matrix fragmentation works
    .reg .u64 a_ptr_for_thread;
    add.u64 a_ptr_for_thread, a_ptr, thread_id_sdf;
    add.u64 a_ptr_for_thread, a_ptr_for_thread, head_a_offset;
    .reg .f32 a_<16>;
    ld.global.v2.f32 {a_0, a_1}, [a_ptr_for_thread];
    ld.global.v2.f32 {a_2, a_3}, [a_ptr_for_thread + 32];
    ld.global.v2.f32 {a_4, a_5}, [a_ptr_for_thread + 64];
    ld.global.v2.f32 {a_6, a_7}, [a_ptr_for_thread + 96];
    ld.global.v2.f32 {a_8, a_9}, [a_ptr_for_thread + 128];
    ld.global.v2.f32 {a_10, a_11}, [a_ptr_for_thread + 160];
    ld.global.v2.f32 {a_12, a_13}, [a_ptr_for_thread + 192];
    ld.global.v2.f32 {a_14, a_15}, [a_ptr_for_thread + 224];

    .reg .u64 input_offset;
    mul.wide.u32 input_offset, thread_id, 32;

    .reg .u32 sequence_index;
    mov.u32 sequence_index, 0;
    .reg .u64 offset_sequence_ptr;
    // offset for our thread. TODO: this is not correct. need to figure out how to do matrix
    // tiling.
    add.u64 offset_sequence_ptr, u_ptr, input_offset;
    .reg .pred should_we_exit_sequence;

    .reg .u64 u_ptr_for_thread;
    // row-major ordering
    add.u64 u_ptr_for_thread, u_ptr, thread_id_times_4;
    add.u64 u_ptr_for_thread, u_ptr_for_thread, head_in_out_offset;
    .reg .u64 output_ptr_for_thread;
    add.u64 output_ptr_for_thread, output_ptr, thread_id_times_4;
    add.u64 output_ptr_for_thread, output_ptr_for_thread, head_in_out_offset;

    // U is {L=8, BATCH_SIZE=16} -> {8, 16}
    .reg .b32 u_0;
    .reg .b32 u_1;

    .reg .f32 x_<32>;
    .reg .b32 x_combined_<16>;

    // Initialize state to 0
    mov.f32 x_0, 0.0;
    mov.f32 x_1, 0.0;
    mov.f32 x_2, 0.0;
    mov.f32 x_3, 0.0;
    mov.f32 x_4, 0.0;
    mov.f32 x_5, 0.0;
    mov.f32 x_6, 0.0;
    mov.f32 x_7, 0.0;
    mov.f32 x_8, 0.0;
    mov.f32 x_9, 0.0;
    mov.f32 x_10, 0.0;
    mov.f32 x_11, 0.0;
    mov.f32 x_12, 0.0;
    mov.f32 x_13, 0.0;
    mov.f32 x_14, 0.0;
    mov.f32 x_15, 0.0;
    mov.f32 x_16, 0.0;
    mov.f32 x_17, 0.0;
    mov.f32 x_18, 0.0;
    mov.f32 x_19, 0.0;
    mov.f32 x_20, 0.0;
    mov.f32 x_21, 0.0;
    mov.f32 x_22, 0.0;
    mov.f32 x_23, 0.0;
    mov.f32 x_24, 0.0;
    mov.f32 x_25, 0.0;
    mov.f32 x_26, 0.0;
    mov.f32 x_27, 0.0;
    mov.f32 x_28, 0.0;
    mov.f32 x_29, 0.0;
    mov.f32 x_30, 0.0;
    mov.f32 x_31, 0.0;

    .reg .f32 out_<4>;
    .reg .b32 out_combined_<2>;

SEQUENCE_MULS:
    // We're loading 4 8x8 matrices such that thread0 (in lane!) has first 8 matrix0 values
    // thread1 has second 8 matrix0 values, thread9 has first 8 matrix1 values, etc.

    // This loading pattern is probably incorrect with matrix fragmentation

    // Load u[:8] in ROW-MAJOR format
    ld.global.b32 u_0, [u_ptr_for_thread];
    // ldmatrix.sync.aligned.shape.num
    // Load u[8:]
    ld.global.b32 u_1, [u_ptr_for_thread+128];
    // ld.global.b32 u_1, [u_ptr];

    // Calculate A*X. A is (N=64,) and X is (B=16, N=64). We want to multiply each value in X by its matching A.
    mul.f32 x_0, x_0, a_0;
    mul.f32 x_1, x_1, a_1;
    mul.f32 x_2, x_2, a_0;
    mul.f32 x_3, x_3, a_1;

    mul.f32 x_4, x_4, a_2;
    mul.f32 x_5, x_5, a_3;
    mul.f32 x_6, x_6, a_2;
    mul.f32 x_7, x_7, a_3;

    mul.f32 x_8, x_8, a_4;
    mul.f32 x_9, x_9, a_5;
    mul.f32 x_10, x_10, a_4;
    mul.f32 x_11, x_11, a_5;
    
    mul.f32 x_12, x_12, a_6;
    mul.f32 x_13, x_13, a_7;
    mul.f32 x_14, x_14, a_6;
    mul.f32 x_15, x_15, a_7;

    mul.f32 x_16, x_16, a_8;
    mul.f32 x_17, x_17, a_9;
    mul.f32 x_18, x_18, a_8;
    mul.f32 x_19, x_19, a_9;
    
    mul.f32 x_20, x_20, a_10;
    mul.f32 x_21, x_21, a_11;
    mul.f32 x_22, x_22, a_10;
    mul.f32 x_23, x_23, a_11;
    
    mul.f32 x_24, x_24, a_12;
    mul.f32 x_25, x_25, a_13;
    mul.f32 x_26, x_26, a_12;
    mul.f32 x_27, x_27, a_13;
    
    mul.f32 x_28, x_28, a_14;
    mul.f32 x_29, x_29, a_15;
    mul.f32 x_30, x_30, a_14;
    mul.f32 x_31, x_31, a_15;


    // Transposed, so row u (B=16, L=8) @ col B (L=8, N=64) in a series of (16, 8) @ (8, 8) chunks. X is now (B=16, N=64)
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_0, x_1, x_2, x_3},     {u_0, u_1}, {b_0}, {x_0, x_1, x_2, x_3}; // (u @ B[:,0:8]) -> X[:,0:8]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_4, x_5, x_6, x_7},     {u_0, u_1}, {b_1}, {x_4, x_5, x_6, x_7}; // (u @ B[:,8:16]) -> X[:,8:16]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_8, x_9, x_10, x_11},   {u_0, u_1}, {b_2}, {x_8, x_9, x_10, x_11}; // (u @ B[:,16:24]) -> X[:,16:24]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_12, x_13, x_14, x_15}, {u_0, u_1}, {b_3}, {x_12, x_13, x_14, x_15}; // (u @ B[:,24:32]) -> X[:,24:32]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_16, x_17, x_18, x_19}, {u_0, u_1}, {b_4}, {x_16, x_17, x_18, x_19}; // (u @ B[:,32:40]) -> X[:,32:40]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_20, x_21, x_22, x_23}, {u_0, u_1}, {b_5}, {x_20, x_21, x_22, x_23}; // (u @ B[:,40:48]) -> X[:,40:48]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_24, x_25, x_26, x_27}, {u_0, u_1}, {b_6}, {x_24, x_25, x_26, x_27}; // (u @ B[:,48:56]) -> X[:,48:56]
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {x_28, x_29, x_30, x_31}, {u_0, u_1}, {b_7}, {x_28, x_29, x_30, x_31}; // (u @ B[:,56:64]) -> X[:,56:64]

    cvt.rz.bf16x2.f32 x_combined_0, x_1, x_0;
    cvt.rz.bf16x2.f32 x_combined_1, x_3, x_2;
    cvt.rz.bf16x2.f32 x_combined_2, x_5, x_4;
    cvt.rz.bf16x2.f32 x_combined_3, x_7, x_6;
    cvt.rz.bf16x2.f32 x_combined_4, x_9, x_8;
    cvt.rz.bf16x2.f32 x_combined_5, x_11, x_10;
    cvt.rz.bf16x2.f32 x_combined_6, x_13, x_12;
    cvt.rz.bf16x2.f32 x_combined_7, x_15, x_14;
    cvt.rz.bf16x2.f32 x_combined_8, x_17, x_16;
    cvt.rz.bf16x2.f32 x_combined_9, x_19, x_18;
    cvt.rz.bf16x2.f32 x_combined_10, x_21, x_20;
    cvt.rz.bf16x2.f32 x_combined_11, x_23, x_22;
    cvt.rz.bf16x2.f32 x_combined_12, x_25, x_24;
    cvt.rz.bf16x2.f32 x_combined_13, x_27, x_26;
    cvt.rz.bf16x2.f32 x_combined_14, x_29, x_28;
    cvt.rz.bf16x2.f32 x_combined_15, x_31, x_30;
    // prmt.b32 x_combined_1, x_2, x_3, 12918; is equivalent

    mov.f32 out_0, 0.0;
    mov.f32 out_1, 0.0;
    mov.f32 out_2, 0.0;
    mov.f32 out_3, 0.0;

    // X (16, 64) @ C (64, 8) -> (16, 8)
    // Each matmul is a (16, 8) @ (8, 8) section
    // First X.T[,:8] @ C[:8] -> (16, 8)
    // Then X[,8:16] @ C[8:16] -> (16, 8)
    // Then X[,16:24] @ C[16:24] -> (16, 8)

    // X[:,0:16] @ C[0:16] -> Out (B=16, L=8)

    // X[:,:8] @ C[:8] -> 
    // mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_combined_0, x_combined_1}, {c_0}, {out_0, out_1, out_2, out_3};
    // mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_combined_2, x_combined_3}, {c_1}, {out_0, out_1, out_2, out_3};
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_combined_0, x_combined_1, x_combined_2, x_combined_3}, {c_0, c_1}, {out_0, out_1, out_2, out_3};
    // X[:,16:32] @ C[16:32]
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_combined_4, x_combined_5, x_combined_6, x_combined_7}, {c_2, c_3}, {out_0, out_1, out_2, out_3};
    // X[:,32:48] @ C[32:48]
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_combined_8, x_combined_9, x_combined_10, x_combined_11}, {c_4, c_5}, {out_0, out_1, out_2, out_3};
    // X[:,48:64] @ C[48:64]
    mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {x_combined_12, x_combined_13, x_combined_14, x_combined_15}, {c_6, c_7}, {out_0, out_1, out_2, out_3};

    // + row u (B=16, L=8) @ D (L=8, L=8) -> (B=16, L=8)
    mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {out_0, out_1, out_2, out_3}, {u_0, u_1}, {d_fragment}, {out_0, out_1, out_2, out_3};

    cvt.rz.bf16x2.f32 out_combined_0, out_1, out_0;
    cvt.rz.bf16x2.f32 out_combined_1, out_3, out_2;

    st.global.b32 [output_ptr_for_thread], out_combined_0;
    st.global.b32 [output_ptr_for_thread+128], out_combined_1;

    sub.u16 sequence_length_small, sequence_length_small, 1;
    add.u64 u_ptr_for_thread, u_ptr_for_thread, 8192; // L=8 BATCH_SIZE=16 N_HEADS=32 * 2 bytes
    add.u64 output_ptr_for_thread, output_ptr_for_thread, 8192; // L=8 BATCH_SIZE=16 N_HEADS=32 * 2 bytes

    setp.gt.u16 should_we_exit_sequence, sequence_length_small, 0;
    @should_we_exit_sequence bra SEQUENCE_MULS;

    ret;
}
